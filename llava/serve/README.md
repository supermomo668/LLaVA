# Model Serving

## Endpoint request format to worker model

### Parameters (originally controlled by Gradio UI)
```
model_selector = gr.Dropdown(
  choices=models,
  value=models[0] if len(models) > 0 else "",
  interactive=True,
  show_label=False,
  container=False)
temperature = gr.Slider(
  minimum=0.0, maximum=1.0, value=0.2, step=0.1,
  interactive=True, label="Temperature")
top_p = gr.Slider(
  minimum=0.0, maximum=1.0, value=0.7, step=0.1, interactive=True, label="Top P")
max_output_tokens = gr.Slider(
  minimum=0, maximum=1024, value=512, step=64,
  interactive=True, label="Max output tokens")
```
Defaults:
* model is loaded from ```load_pretrained_model``` in ```serve/model_worker.py``` where name is adjusted by model path and passed to load desired model in ```model/builder.py```:
```
args.model_name=liuhaotian/llava-llama-2-13b-chat-lightning-preview
args.model_base=None
```
* Other parameters (as defined in UI):
```
temperature=0.2
top-P=0.7
max_output_tokens=512
```

### Prompt Request
Located in ```gradio_web_server.py```

```
pload = {
    "model": model_name,
    "prompt": prompt,
    "temperature": float(temperature),
    "top_p": float(top_p),
    "max_new_tokens": min(int(max_new_tokens), 1536),
    "stop": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,
    "images": f'List of {len(state.get_images())} images: {all_image_hash}',
}
logger.info(print_pload(pload))
logger.info(f"==== request ====\n{pload}")

pload['images'] = state.get_images()

state.messages[-1][-1] = "▌"
yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5

try:
  # Stream output
  response = requests.post(
      worker_addr + "/worker_generate_stream",
      headers=headers, json=pload, stream=True, timeout=10
  )
  for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\0"):
    if chunk:
      data = json.loads(chunk.decode())
      if data["error_code"] == 0:
          output = data["text"][len(prompt):].strip()
          state.messages[-1][-1] = output + "▌"
          yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5
      else:
          output = data["text"] + f" (error_code: {data['error_code']})"
          state.messages[-1][-1] = output
```

## Key Pointers

In [llava.gradio_web_server.py](gradio_web_server.py)
* ```state``` is defined in ```load_demo_refresh_model_list``` 
  ```
  state = default_conversation.copy()
  ```
  `Conversation` class is used to keep track of the conversation 
  
* Request/response to/from workers is generated by:
  ```
  response = requests.post(
    worker_addr + "/worker_generate_stream",
    headers=headers, json=pload, stream=True, timeout=10
  )
  ```

In [llava.conversation.py](../conversation.py) provides 
  
  * ```to_gradio_chatbot```:  KEY processing function of Input Payload
  * the base prompts for initializing and connecting between prompts.
  
# Key Doc Reference

- [Block and Event listners](https://www.gradio.app/main/guides/blocks-and-event-listeners)